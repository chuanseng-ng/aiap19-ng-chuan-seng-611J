{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze problem statement to identify approach to fulfill goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Company faces significant challenges in optimizing crop yields and resource management <br>\n",
    "Need to prioritize focus on these 2 objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Predict temperature conditions within farm's closed environment, ensuring optimal plant growth\n",
    "  - Regression modelling task\n",
    "  - Need to identify relevant/related features within provided database\n",
    "<br><br>\n",
    "2. Categorize combined \"Plant Type-Stage\" based on sensor data, aiding in strategic planning and resource allocation\n",
    "  - Classification modelling task\n",
    "  - Need to identify relevant/related features within provided database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment, SQL connection and analyze SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary libraries will be imported when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish connection SQL database (agri.db) using relative path 'data/agri.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries as needed\n",
    "import sqlite3\n",
    "\n",
    "# Set path to SQL database\n",
    "db_path = \"data/agri.db\"\n",
    "\n",
    "# Create connection to SQL database\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set pandas options for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Display all columns in DataFrame\n",
    "pd.set_option('display.max_rows', 100)     # Limit number of rows displayed to 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore database structure by listing all tables to identify available tables for extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Query to list all tables in database\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql(query, conn)\n",
    "\n",
    "# Display list of tables\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only 'farm_data' table in the database, the first few rows can be previewed to understand the column structure <br>\n",
    "This can be cross-checked with the provided list of attributes in the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few rows of 'farm_data' table\n",
    "farm_data_10_query = \"SELECT * FROM farm_data LIMIT 10;\"\n",
    "farm_data_10_df = pd.read_sql(farm_data_10_query, conn)\n",
    "\n",
    "# Display first few rows of table\n",
    "farm_data_10_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few issues in the current database that will have to be sorted out before the data can be used for feature engineering or used in machine learning modelling <br>\n",
    "<br>\n",
    "Currently identified issues:\n",
    "| Column Name | Issue |\n",
    "| :---------: | :---: |\n",
    "| Plant Type  | Non-standardized naming format |\n",
    "| Plant Stage | Non-standardized naming format |\n",
    "| Temperature Sensor | Missing values (NaN) |\n",
    "|                    | Negative value |\n",
    "| Humidity Sensor | Missing values (NaN) |\n",
    "| Nutrient * Sensor | Missing value (None) |\n",
    "|                   | Values with units |\n",
    "| Water Level Sensor | Missing value (NaN) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema of 'farm_data' table can be retrieved to understand the columns' data type <br>\n",
    "However, this might not be fully accurate prior to data clean-up due to missing or incorrectly labelled values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema of 'farm_data' table\n",
    "schema_query = \"PRAGMA table_info(farm_data);\"\n",
    "schema_df = pd.read_sql(schema_query, conn)\n",
    "\n",
    "# Display schema information\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the columns' data type match expectations, except for Nutrient Sensors <br>\n",
    "These columns should have REAL/INTEGER type but are currently of TEXT type <br> <br>\n",
    "To handle this, the missing values will have to resolved and the values with units need to be processed <br>\n",
    "After both steps are done, the columns' data can be converted to REAL/INTEGER type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Exploratory Data Analysis (EDA) on SQL table data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all data from 'farm_data' table into a DataFrame to start data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data from 'farm_data' table\n",
    "farm_data_query = \"SELECT * FROM farm_data;\"\n",
    "farm_data_df = pd.read_sql_query(farm_data_query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with data preprocessing to clean-up missing values, non-standardized naming format and extra info in values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in columns (Plant Type, Plant Stage) will all be changed to lowercase characters to standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_standard_name_list = [\"Plant Type\", \"Plant Stage\"]\n",
    "\n",
    "for col_name in non_standard_name_list:\n",
    "    farm_data_df[col_name] = farm_data_df[col_name].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the classification task, 'Plant Type' and 'Plant Stage' columns will need to be merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Double check on implementation\n",
    "#farm_data_df[\"Plant Type-Stage\"] = farm_data_df[\"Plant Type\"] + \" \" + farm_data_df[\"Plant Stage\"]\n",
    "#farm_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 'ppm' from Nutrient * Sensor column data and convert the data into numeric type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppm_drop_list = [\"Nutrient N Sensor (ppm)\", \"Nutrient P Sensor (ppm)\", \"Nutrient K Sensor (ppm)\"]\n",
    "\n",
    "for col_name in ppm_drop_list:\n",
    "    farm_data_df[col_name] = farm_data_df[col_name].str.replace(\"ppm\", \"\", regex=False)\n",
    "    farm_data_df[col_name] = pd.to_numeric(farm_data_df[col_name], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove negative sign in Temperature Sensor column data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Temperature Sensor column name\n",
    "farm_data_df_col_list = farm_data_df.columns\n",
    "\n",
    "for col_name in farm_data_df_col_list:\n",
    "    if \"Temperature Sensor\" in col_name:\n",
    "        temp_sensor_col_name = col_name\n",
    "\n",
    "farm_data_df[temp_sensor_col_name] = farm_data_df[temp_sensor_col_name].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the current DataFrame, it seems that Light Intensity Sensor column also has missing values <br>\n",
    "So that will be handled together with the other affected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining columns with missing values, they will be filled with either mean or median values of their zone <br>\n",
    "The existing data of each column will be grouped into their System Location Code to obtain the mean and median values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the breakdown of each column:\n",
    "| Column Name | Mean/Median | Reason |\n",
    "| :---------: | :---------: | :----: |\n",
    "| Temperature Sensor | Mean | Data is relatively stable and has normal distribution |\n",
    "| Humidity Sensor | Median  | Data is not normally distributed, no clear pattern |\n",
    "| Light Intensity Sensor | Median | Data is skewed towards the upper half of the spectrum |\n",
    "| Nutrient N Sensor | Median | Data has sudden dip and spike near right of spectrum |\n",
    "| Nutrient P Sensor | Median | Data has sudden dip and spike near left of spectrum |\n",
    "| Nutrient K Sensor | Median | Data has sudden dip and spike near right of spectrum |\n",
    "| Water Level Sensor | Median | To avoid outliers at extreme ends of spectrum |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and median of each column with missing value is still calculated and displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_list = [\"mean\", \"median\"]\n",
    "no_nan_col_list = farm_data_df.columns[farm_data_df.isnull().sum() == 0].tolist()\n",
    "# Don't drop 'System Location Code' column else there is no zone to groupby\n",
    "no_nan_col_list = [col for col in no_nan_col_list if col != \"System Location Code\"]\n",
    "\n",
    "nan_farm_data_df =  farm_data_df.drop(columns=no_nan_col_list)\n",
    "\n",
    "nan_farm_data_grouped_df = nan_farm_data_df.groupby(\"System Location Code\").agg(agg_list)\n",
    "\n",
    "nan_farm_data_grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that for each column, the mean and median values differ by some margin <br>\n",
    "So choosing the appropriate one to replace missing value is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values of each column will be replaced as showed in the above table <br>\n",
    "Change approach of handling missing value in 'Temperature Sensor' to drop missing values instead of replace with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame for data after removing missing values\n",
    "clean_farm_data_df = farm_data_df\n",
    "\n",
    "# Replace 'Temperature Sensor' column missing value with mean\n",
    "#clean_farm_data_df[temp_sensor_col_name] = clean_farm_data_df[temp_sensor_col_name].fillna(clean_farm_data_df[temp_sensor_col_name].mean())\n",
    "\n",
    "nan_col_list = clean_farm_data_df.columns[clean_farm_data_df.isnull().any()].tolist()\n",
    "# Remove 'Temperature Sensor' column name from list as it uses mean instead of median\n",
    "nan_col_list = [col for col in nan_col_list if col != temp_sensor_col_name]\n",
    "\n",
    "# Replace remaining affected column missing value with median\n",
    "for col_name in nan_col_list:\n",
    "    clean_farm_data_df[col_name] = clean_farm_data_df[col_name].fillna(clean_farm_data_df[col_name].median())\n",
    "\n",
    "# Drop all rows in 'Temperature Sensor' column with missing values\n",
    "clean_farm_data_df = clean_farm_data_df.dropna().reset_index()\n",
    "\n",
    "clean_farm_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the current data and data types, each column can be categorized as categorical or numerical types <br>\n",
    "Categorical represents categories or labels, so usually the data are of string or object data type <br>\n",
    "Numerical represents quantiative data, which can be either continuous or discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the breakdown:\n",
    "| Column Name | Type |\n",
    "| :---------: | :--: |\n",
    "| System Location Code | Categorical |\n",
    "| Previous Cycle Plant Type | Categorical |\n",
    "| Plant Type | Categorical |\n",
    "| Plant Stage | Categorical |\n",
    "| Temperature Sensor | Numerical |\n",
    "| Humidity Sensor | Numerical |\n",
    "| Light Intensity Sensor | Numerical |\n",
    "| CO2 Sensor | Numerical |\n",
    "| EC Sensor | Numerical |\n",
    "| O2 Sensor | Numerical |\n",
    "| Nutrient * Sensor | Numerical |\n",
    "| pH Level | Numerical |\n",
    "| Water Level Sensor | Numerical |\n",
    "| Plant Type-Stage | Categorical |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the categorical variables are plotted for visualization <br>\n",
    "It is to get a sense of how the data is categorized and the evenness of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#cat_col_list = [\"System Location Code\", \"Previous Cycle Plant Type\", \"Plant Type\", \"Plant Stage\", \"Plant Type-Stage\"]\n",
    "cat_col_list = [\"System Location Code\", \"Previous Cycle Plant Type\", \"Plant Type\", \"Plant Stage\"]\n",
    "\n",
    "for col_name in cat_col_list:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.countplot(x=col_name, data=clean_farm_data_df)\n",
    "    plt.title(f\"Distribution of {col_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the data distribution of all 4 columns are rather even across all the distinct values <br>\n",
    "It means that there is no need to do any further data processing to balance out skewed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns will need to have their data values converted into categorical numeric values via label categorization and/or one-hot encoding<br>\n",
    "Else it is not possible to use these columns for correlation analysis and machine learning modelling in later steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create new DataFrame for post-encoding\n",
    "encoded_farm_data_df = clean_farm_data_df\n",
    "\n",
    "# Perform label encoding on 'Plant Stage' as there is an ordered stage to it\n",
    "#lab_enc_list = [\"Plant Stage\", \"Plant Type-Stage\"]\n",
    "lab_enc_list = [\"Plant Stage\"]\n",
    "lab_enc = LabelEncoder()\n",
    "for col_name in lab_enc_list:\n",
    "    encoded_farm_data_df[col_name] = lab_enc.fit_transform(encoded_farm_data_df[col_name])\n",
    "\n",
    "# Perform one-hot encoding on the other columns as there is no order\n",
    "fil_cat_col_list = [item for item in cat_col_list if item not in lab_enc_list]\n",
    "encoded_farm_data_df = pd.get_dummies(encoded_farm_data_df, columns=fil_cat_col_list, drop_first=True)\n",
    "bool_col = encoded_farm_data_df.select_dtypes(include=[\"bool\"]).columns\n",
    "encoded_farm_data_df[bool_col] = encoded_farm_data_df[bool_col].astype(int)\n",
    "encoded_farm_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution and relationship of numerical variables are plotted for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col_list = [item for item in farm_data_df_col_list if item not in cat_col_list]\n",
    "\n",
    "for col_name in num_col_list:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.histplot(clean_farm_data_df[col_name], kde=True)\n",
    "    plt.title(f\"Distribution of {col_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After replacing the missing values, there is a sharp sudden spike in the median region for the affected features <br>\n",
    "This helps to create a normal distribution in the features but due to the number of missing values replaced creating a sharp spike, the impact will have to be assessed in the later stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to humidity having such a big bias in the median range and having too many missing values, the column should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_col_list = [\"Humidity Sensor (%)\"]\n",
    "clean_farm_data_df = clean_farm_data_df.drop(columns=drop_col_list)\n",
    "encoded_farm_data_df = encoded_farm_data_df.drop(columns=drop_col_list)\n",
    "\n",
    "num_col_list = [item for item in num_col_list if item not in drop_col_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numerical values will need to be standardized via standard scaling <br>\n",
    "This makes the features have mean of 0 and standard deviation of 1 <br>\n",
    "This helps the algorithm perform better as the input features would be on a similar scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize numerical data-set\n",
    "scaler = StandardScaler()\n",
    "encoded_farm_data_df[num_col_list] = scaler.fit_transform(encoded_farm_data_df[num_col_list])\n",
    "encoded_farm_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze patterns and distribution in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Predict temperature conditions within farm's closed environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot heatmap for visualization to perform dimension reduction in latter steps <br>\n",
    "Dimension reduction is needed to eliminate redundant/relevant data that are not important in predicting/classifying the expected outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = encoded_farm_data_df.corr()\n",
    "\n",
    "# Create heatmap of correlation matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.title(\"Correlation matrix heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on 'Temperature Sensor' first <br>\n",
    "Print out correlation values with relation to 'Temperature Sensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sens_corr = corr_matrix[temp_sensor_col_name]\n",
    "\n",
    "# Sort correlations by absolute value (if strong correlations should be prioritized)\n",
    "sorted_temp_sens_corr = temp_sens_corr.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Correlation with 'Temperature Sensor': \")\n",
    "print(sorted_temp_sens_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most correlation values are lesser than 0.1, the threshold value to drop features will be set at 0.1 <br>\n",
    "This applies for 'Temperature Sensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_threshold = 0.1\n",
    "\n",
    "temp_sens_drop_col = sorted_temp_sens_corr[sorted_temp_sens_corr < corr_threshold].index\n",
    "\n",
    "temp_sens_farm_data_df = encoded_farm_data_df.drop(columns=temp_sens_drop_col, axis=1)\n",
    "temp_sens_farm_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check correlation matrix of 'Temperature Sensor' again for the features present after dropping lower correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix again\n",
    "final_temp_sens_corr_matrix = temp_sens_farm_data_df.corr()\n",
    "\n",
    "# Create heatmap of correlation matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(final_temp_sens_corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.title(\"Temperature sensor correlation matrix heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the numerical correlation values for 'Temperature Sensor' after dropping lower correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_temp_sens_corr = final_temp_sens_corr_matrix[temp_sensor_col_name]\n",
    "\n",
    "# Sort correlations by absolute value (if strong correlations should be prioritized)\n",
    "final_sorted_temp_sens_corr = final_temp_sens_corr.abs().sort_values(ascending=False)\n",
    "\n",
    "# Print numerical correlation values\n",
    "print(\"Correlation with 'Temperature Sensor': \")\n",
    "print(final_sorted_temp_sens_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DataFrame with 'Temperature Sensor' as the target feature, split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split DataFrame into target feature and correlated features\n",
    "X = temp_sens_farm_data_df.drop([temp_sensor_col_name], axis=1) # Correlated features\n",
    "Y = temp_sens_farm_data_df[temp_sensor_col_name]                # Target feature\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into test and train sets (20-80 split)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with Linear Regression to train model and check on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "lin_Y_predict = lin_model.predict(X_test)\n",
    "\n",
    "lin_mse = mean_squared_error(Y_test, lin_Y_predict)\n",
    "lin_r2 = r2_score(Y_test, lin_Y_predict)\n",
    "\n",
    "print(\"Pre-tuned Linear Regression - \")\n",
    "print(f\"Mean Squared Error: {lin_mse}\")\n",
    "print(f\"R2 Score: {lin_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Linear Regression results, the mean squared error and R2 score values are not optimal <br>\n",
    "Expectation - <br>\n",
    "- Mean squared error value close to 0\n",
    "- R2 score value close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out with Random Forest Regression to see if there are better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rand_for_reg_model = RandomForestRegressor()\n",
    "rand_for_reg_model.fit(X_train, Y_train)\n",
    "rand_for_reg_Y_predict = rand_for_reg_model.predict(X_test)\n",
    "\n",
    "rand_for_reg_mse = mean_squared_error(Y_test, rand_for_reg_Y_predict)\n",
    "rand_for_reg_r2 = r2_score(Y_test, rand_for_reg_Y_predict)\n",
    "\n",
    "print(\"Pre-tuning Random Forest Regression - \")\n",
    "print(f\"Mean Squared Error: {rand_for_reg_mse}\")\n",
    "print(f\"R2 Score: {rand_for_reg_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Regression results with default parameters are noticibly better than Linear Regression <br>\n",
    "However, the results can be improved by tweaking the parameters via GridSearchCV or RandomSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune Random Forest Regression for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_n_eval_forest_regression(X_train, Y_train, X_test, Y_test, search_method = \"grid\", param_grid = None, param_dist = None, random_iter = 50, cv = 5, num_jobs = 4):\n",
    "    \"\"\"\n",
    "    Automates the tuning and evaluation of a Random Forest Regression model.\n",
    "\n",
    "    Parameters:\n",
    "        X: Features (DataFrame or array).\n",
    "        y: Target variable (Series or array).\n",
    "        search_method: 'grid' for GridSearchCV, 'random' for RandomizedSearchCV.\n",
    "        param_grid: Dictionary of hyperparameter ranges for GridSearchCV.\n",
    "        param_dist: Dictionary of hyperparameter distributions for RandomizedSearchCV.\n",
    "        random_iter: Number of iterations for RandomizedSearchCV.\n",
    "        cv: Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "        best_model: The tuned Random Forest Regression model.\n",
    "        best_params: The best hyperparameters found.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "    # Initialize parameters\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            \"n_estimators\": [50, 100, 150, 200],\n",
    "            \"max_depth\": [None, 5, 10]\n",
    "        }\n",
    "    if param_dist is None:\n",
    "        from scipy.stats import randint\n",
    "        param_dist = {\n",
    "            \"n_estimators\": randint(100, 300),\n",
    "            \"max_depth\": [None, 5, 10, 15]\n",
    "        }\n",
    "\n",
    "    if search_method == \"grid\":\n",
    "        search = GridSearchCV(\n",
    "            RandomForestRegressor(random_state = 42),\n",
    "            param_grid = param_grid,\n",
    "            cv = cv,\n",
    "            scoring = \"neg_mean_squared_error\",\n",
    "            n_jobs = num_jobs\n",
    "        )\n",
    "    elif search_method == \"random\":\n",
    "        search = RandomizedSearchCV(\n",
    "            RandomForestRegressor(random_state = 42),\n",
    "            param_distributions = param_dist,\n",
    "            n_iter = random_iter,\n",
    "            cv = cv,\n",
    "            scoring = \"neg_mean_squared_error\",\n",
    "            random_state = 42,\n",
    "            n_jobs = num_jobs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"search_method must be either 'grid' or ' random'\")\n",
    "    \n",
    "    # Fit the search\n",
    "    print(f\"Running {search_method.capitalize()} Search...\")\n",
    "    search.fit(X_train, Y_train)\n",
    "\n",
    "    # Best model and parameters\n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "    print(f\"\\nBest Parameters: {best_params}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    tuned_rand_for_reg_Y_predict = best_model.predict(X_test)\n",
    "    tuned_rand_for_reg_mse = mean_squared_error(Y_test, tuned_rand_for_reg_Y_predict)\n",
    "    tuned_rand_for_reg_r2 = r2_score(Y_test, tuned_rand_for_reg_Y_predict)\n",
    "    print(\"Tuned Random Forest Regression -\")\n",
    "    print(f\"Tuned Mean Squared Error: {tuned_rand_for_reg_mse}\")\n",
    "    print(f\"Tuned R2 Score: {tuned_rand_for_reg_r2}\")\n",
    "\n",
    "\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call tuning function and evaluate function <br>\n",
    "Run function with both GridSearchCV and RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Grid Search\n",
    "grid_rand_for_best_model, grid_rand_for_best_param = tune_n_eval_forest_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    search_method = \"grid\",\n",
    "    num_jobs = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Search\n",
    "rand_rand_for_best_model, rand_rand_for_best_param = tune_n_eval_forest_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    search_method = \"random\",\n",
    "    num_jobs = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there isn't a noticable difference in metrics after tuning, the overall result of the Random Forest Regression is the best at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out with XGBoost to see how gradient boosting framework performs for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg_model = XGBRegressor(random_state=42)\n",
    "xgb_reg_model.fit(X_train, Y_train)\n",
    "xgb_reg_Y_predict = xgb_reg_model.predict(X_test)\n",
    "\n",
    "xgb_reg_mse = mean_squared_error(Y_test, xgb_reg_Y_predict)\n",
    "xgb_reg_r2 = r2_score(Y_test, xgb_reg_Y_predict)\n",
    "\n",
    "print(\"XGBoost Regression - \")\n",
    "print(f\"Mean Squared Error: {xgb_reg_mse}\")\n",
    "print(f\"R2 Score: {xgb_reg_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Regression currently performs somewhere in between Linear Regression and Random Forest Regression <br>\n",
    "XGBoost Regression's parameters can also be tuned to help improve the metric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_n_eval_xgb_regression(X_train, Y_train, X_test, Y_test, search_method = \"grid\", param_grid = None, param_dist = None, random_iter = 50, cv = 5, num_jobs = 4):\n",
    "    \"\"\"\n",
    "    Automates the tuning and evaluation of a XGBoost Regression model.\n",
    "\n",
    "    Parameters:\n",
    "        X: Features (DataFrame or array).\n",
    "        y: Target variable (Series or array).\n",
    "        search_method: 'grid' for GridSearchCV, 'random' for RandomizedSearchCV.\n",
    "        param_grid: Dictionary of hyperparameter ranges for GridSearchCV.\n",
    "        param_dist: Dictionary of hyperparameter distributions for RandomizedSearchCV.\n",
    "        random_iter: Number of iterations for RandomizedSearchCV.\n",
    "        cv: Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "        best_model: The tuned Random Forest Regression model.\n",
    "        best_params: The best hyperparameters found.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "    # Initialize parameters\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            \"n_estimators\": [100, 200, 300],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"subsample\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "    if param_dist is None:\n",
    "        from scipy.stats import randint\n",
    "        param_dist = {\n",
    "            \"n_estimators\": randint(100, 300),\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"subsample\": [0.6, 0.8, 1.0]\n",
    "        }\n",
    "\n",
    "    if search_method == \"grid\":\n",
    "        search = GridSearchCV(\n",
    "            XGBRegressor(random_state = 42),\n",
    "            param_grid = param_grid,\n",
    "            cv = cv,\n",
    "            scoring = \"neg_mean_squared_error\",\n",
    "            n_jobs = num_jobs\n",
    "        )\n",
    "    elif search_method == \"random\":\n",
    "        search = RandomizedSearchCV(\n",
    "            XGBRegressor(random_state = 42),\n",
    "            param_distributions = param_dist,\n",
    "            n_iter = random_iter,\n",
    "            cv = cv,\n",
    "            scoring = \"neg_mean_squared_error\",\n",
    "            random_state = 42,\n",
    "            n_jobs = num_jobs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"search_method must be either 'grid' or ' random'\")\n",
    "    \n",
    "    # Fit the search\n",
    "    print(f\"Running {search_method.capitalize()} Search...\")\n",
    "    search.fit(X_train, Y_train)\n",
    "\n",
    "    # Best model and parameters\n",
    "    best_model = search.best_estimator_\n",
    "    best_params = search.best_params_\n",
    "    print(f\"\\nBest Parameters: {best_params}\")\n",
    "\n",
    "    # Test set evaluation\n",
    "    tuned_xgb_reg_Y_predict = best_model.predict(X_test)\n",
    "    tuned_xgb_reg_mse = mean_squared_error(Y_test, tuned_xgb_reg_Y_predict)\n",
    "    tuned_xgb_reg_r2 = r2_score(Y_test, tuned_xgb_reg_Y_predict)\n",
    "    print(\"Tuned XGBoost Regression -\")\n",
    "    print(f\"Tuned Mean Squared Error: {tuned_xgb_reg_mse}\")\n",
    "    print(f\"Tuned R2 Score: {tuned_xgb_reg_r2}\")\n",
    "\n",
    "\n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call tuning function and evaluate function <br>\n",
    "Run function with both GridSearchCV and RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Grid Search\n",
    "grid_rand_for_best_model, grid_rand_for_best_param = tune_n_eval_xgb_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    search_method = \"grid\",\n",
    "    num_jobs = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Search\n",
    "grid_rand_for_best_model, grid_rand_for_best_param = tune_n_eval_xgb_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    search_method = \"random\",\n",
    "    num_jobs = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the parameters for XGBoost Regression, it looks like there is no noticable difference in metric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance and range of the target variable - 'Temperature Sensor' can be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance of target variable\n",
    "variance_temp_sens = Y_test.var()\n",
    "print(f\"Variance of 'Temperature Sensor': {variance_temp_sens:.3f}\")\n",
    "\n",
    "# Calculate range of target variable\n",
    "range_temp_sens = Y_test.max() - Y_test.min()\n",
    "print(f\"Range of 'Temperature Sensor': {range_temp_sens:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run algorithm with PCA (95% variance) features <br>\n",
    "The purpose of using PCA (Principal Component Analysis) is to reduce the dimensionality of the feature space while retaining as much variance as possible <br>\n",
    "It can help to improve model performance by reducing noise, handling multicollinearity, and improving training speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca_temp_sens = PCA(n_components=0.95) # Keep 95% of variance\n",
    "X_pca = pca_temp_sens.fit_transform(X)\n",
    "\n",
    "print(\"Explained variance ratio: \", pca_temp_sens.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance: \", np.cumsum(pca_temp_sens.explained_variance_ratio_))\n",
    "\n",
    "pca_temp_sens_X_train, pca_temp_sens_X_test, pca_temp_sens_Y_train, pca_temp_sens_Y_test = train_test_split(X_pca, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "pca_temp_sens_model = RandomForestRegressor()\n",
    "pca_temp_sens_model.fit(pca_temp_sens_X_train, pca_temp_sens_Y_train)\n",
    "pca_temp_sens_Y_predict = pca_temp_sens_model.predict(pca_temp_sens_X_test)\n",
    "pca_temp_sens_mse = mean_squared_error(Y_test, pca_temp_sens_Y_predict)\n",
    "pca_temp_sens_r2 = r2_score(Y_test, pca_temp_sens_Y_predict)\n",
    "\n",
    "print(\"PCA Random Forest Regression - \")\n",
    "print(f\"Mean Squared Error: {pca_temp_sens_mse}\")\n",
    "print(f\"R2 Score: {pca_temp_sens_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross-validation with PCA feature <br>\n",
    "Purpose of cross-validation is to evaluate performance of a machine learning model on unseen data <br>\n",
    "This helps to avoid overfitting by training and testing the model on different subsets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rand_for_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "temp_sens_pipeline = Pipeline([\n",
    "    (\"pca\", pca_temp_sens),\n",
    "    (\"rf\", rand_for_model)\n",
    "])\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(temp_sens_pipeline, X, Y, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "cv_r2 = cross_val_score(temp_sens_pipeline, X, Y, cv=5, scoring=\"r2\")\n",
    "\n",
    "print(f\"Cross-validation MSE/Fold: {-cv_scores}\")\n",
    "print(f\"Cross-validation Mean MSE: {-cv_scores.mean():.4f}\")\n",
    "print(f\"Cross-validation STD MSE: {-cv_scores.std():.4f}\")\n",
    "print(f\"Cross-validation R2 Score/Fold: {cv_r2}\")\n",
    "print(f\"Cross-validation Mean R2 Score: {cv_r2.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Categorize combined \"Plant Type-Stage\" based on sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, the DataFrame used will be mostly reused from Part 1's prior to any row or column dropping, and feature engineering <br>\n",
    "This is done to preserve as much of the original content as possible for a prelimenary assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated in the requirements, 'Plant Type' and 'Plant Stage' will need to be combined to form a new columns named 'Plant Type-Stage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_data_df[\"Plant Type-Stage\"] = farm_data_df[\"Plant Type\"] + \" \" +  farm_data_df[\"Plant Stage\"]\n",
    "farm_data_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
