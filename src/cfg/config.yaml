database:
  path: "../data/agri.db"             # SQL DataBase path (relative/absolute path is ok)

features:
  part1:                              # Part 1 (Regression)
    target: "Temperature Sensor"      # Target variable column name
    enc_list:                         # List of column names for label encoding
      - "Plant Stage"
    corr_threshold: 0.1               # Correlation threshold value
  part2:                              # Part 2 (Classification)
    target_list:                      # Accepts 2 items as input in list to combine into new column
    - "Plant Type"
    - "Plant Stage"
    target_comb: "Plant Type-Stage"   # Combined variable column name - Used as target variable column name
    corr_threshold: 0.095             # Correlation threshold value
  low_case_list:                      # List of column names to convert data to lower-case
    - "Plant Type"
    - "Plant Stage"
  nutrient_list:                      # List of column names for 'Nutrient * Sensor'
    - "Nutrient N Sensor (ppm)"
    - "Nutrient P Sensor (ppm)"
    - "Nutrient K Sensor (ppm)"
  drop_list:                          # List of column names to drop column
    - "Humidity Sensor (%)"

model:
  test_size: 0.2                      # Decimal value to determine test/train data set splitting metric
  random_state: 42                    # Random seed value
  search_method: "grid"               # Valid values are "grid/random"
  cv_num: 5                           # Number of cross-validation folds
  num_iter: 50                        # Number of iterations for RandomSearchCV 
  num_jobs: -1                        # 0 to n -> Use n number of CPUs/Logical Processors, -1 -> Use all available resources
  part1:
    task_type: "Regression"           # Part 1 task type
    scoring: "neg_mean_squared_error" # Model scoring method during GridSearchCV/RandomizedSearchCV
    model_name_list:                  # Supported regression model list
      - "Linear Regression"           # Simple and fast (Limited to linear relationships)
      - "Random Forest"               # Suitable for complex, non-linear data (Computational expensive)
      - "SVR"                         # Effective at capturing non-linear relationships (Sensitive to hyperparameters and is slow)
      - "MLP"                         # Powerful for complex non-linear data (Requires large data, is slow and is prone to overfitting)
      - "Bayesian Ridge"              # Useful when need probabilistic view of models' uncertainity (Assumes linearity and is slower than linear regression)
      - "XGBoost"                     # High performance model that can perform self-regularization and is fast (Requires careful tuning)
    save_model: True                  # Model save function (True/False)
  part2:
    task_type: "Classification"       # Part 2 task type
    scoring: "accuracy"               # Model scoring method during GridSearchCV/RandomizedSearchCV
    model_name_list:                  # Supported classification model list
      - "Logistic Regression"         # Ideal for simple binary classification problem with linearly separable data (Cannot handle non-linearity)
      - "Random Forest"               # Can model non-linear relationship (Computationally expensive)
      - "SVC"                         # Suitable for high-dimensional spaces with clear margin between classes (Sensitive to tuning and slow for large datasets)
      - "MLP"                         # Highly flexible and powerful for non-linear tasks (Requires large data and is prone to overfitting)
      - "Naive Bayes"                 # Fast and simple, and ideal for text classification tasks (Assumes feature are independent of each other and less flexible)
      - "XGBoost"                     # High accuracy model that can perform self-regularization (Requires careful tuning)
    save_model: False                 # Model save function (True/False)

part1:
  rand_forest:                        # Random Forest Regressor parameters
    est_list:                         # Number of trees in random forest
      - 100
      - 125
      - 150
      - 175
      - 200
      - 225
      - 250
      - 275
      - 300
    depth_list:                       # Depth of each decision tree (How deep each tree can grow)
      - 5
      - 10
      - 15
      - 20
      - 25
      - 30

  svr:                                # Support Vector Regression parameters
    c_list:                           # Controls trade-off between bias and variance - Higher C = more flexibility, but higher risk of overfitting
      - 0.1
      - 1
      - 5
      - 10
      - 15
      - 20
    kernel_list:                      # Type of transformation applied to input features
      - "linear"                      # Similar to linear regression
      - "rbf"                         # Radial basis function - Effective for non-linear
      - "sigmoid"                     # Similar to neural network

  mlp:                                # Multi-layer Perceptron Regressor parameters
    hidden_layer_sizes_list:          # Number and size of hidden layers (Affects model complexity)
      - [50, 50]
      - [100, 50]
      - [100, 100]
    activation_list:                  # Determines non-linearity introduced by activation function (Affects learning capability)
      - "relu"                        # Works best for most hidden layers due to faster convergence 
      - "tanh"                        # Used in networks where centered activations are important
    solver_list:                      # Optimization algorithm
      - "adam"                        # Works better with default settings for more complex models, with faster convergence and better noisy gradient handling
      - "sgd"                         # Simple and can be effective when tuned correctly, especially for smaller or simpler models
    learning_rate_list:               # Controls how learning rate changes during training (Only for 'sgd' solver)
      - "constant"
      - "adaptive"
    max_iter_list:                    # Specifies maximum number of iterations for training
      - 500
      - 1000
      - 1500
      - 2000

  bayes:                              # Bayesian Ridge parameters
    max_iter_list:                    # Controls maximum number of iterations for convergence
      - 300
      - 500
      - 1000
      - 1200
      - 1500
    alpha_1_list:                     # Regularize model's coefficients (Shrink to avoid overfitting)
      - 0.000001
      - 0.00001
      - 0.0001
      - 0.001
    alpha_2_list:                     # Regularize model's variance (Balance model's ability to fit data without overfitting)
      - 0.000001
      - 0.00001
      - 0.0001
      - 0.001

  xgb:                                # XGBoost Regressor parameters
    n_est_list:                       # Controls number of boosting round/trees (More trees can lead to better performance, but risk of overfitting)
      - 100
      - 200
      - 300
      - 400
      - 500
    learning_rate_list:               # Control step size at each boosting round/tree (Smaller learning rate require more trees but helps prevent overfitting)
      - 0.01
      - 0.05
      - 0.1
    max_depth_list:                   # Controls maximum depth of individual trees (Deeper trees can capture more complexity, but may overfit data)
      - 3
      - 5
      - 7
      - 10
    subsample_list:                   # Controls fraction of training data used to build each tree (Introduces randomness)
      - 0.6
      - 0.8
      - 1.0

                                      # L1 regularization - Encourages sparsity -- Best used for feature selection or when only a few important features are expected
                                      # L2 regularization - Shrinks coefficients towards 0 without eliminating any -- Best for preventing overfitting where all features are expected to contribute

part2:
  logistic:                           # Logistic Regression parameters
    solver_list:                      # Determines optimization algorithm used to fit model
      - "lbfgs"                       # Best for large datasets with L2 regularization
      - "liblinear"                   # Use for small datasets or when L1 regularization is needed
      - "sag"                         # Ideal for very large datasets or sparse data, especially with L3 regularization
      - "newton-cg"                   # Choose for medium-sized datasets with L2 regularization
      - "newton-cholesky"             # Use for small to medium-sized L2 regularization problems
    max_iter_list:                    # Specifies maximum number of iterations for solver
      - 100
      - 125
      - 150
      - 175
      - 200
    c_list:                           # Controls regularization strength (Smaller values increase regularization)
      - 0.1
      - 1
      - 5
      - 10
      - 15
      - 20
    class_weight_list:                # Helps handle imbalanced class distributions by adjusting class weight
      - "balanced"
      - {0: 1, 1: 1.7}
      - {0: 1, 1: 2}
      - {0: 1, 1: 5}

  rand_forest:                        # Random Forest Classifier parameters
    est_list:                         # Number of trees in random forest
      - 100
      - 125
      - 150
      - 175
      - 200
      - 225
      - 250
      - 275
      - 300
    depth_list:                       # Depth of each decision tree (How deep each tree can grow)
      - 5
      - 10
      - 15
      - 20
      - 25
      - 30
    class_weight_list:                # Helps handle imbalanced class distributions by adjusting class weight
      - "balanced"
      - {0: 1, 1: 1.7}
      - {0: 1, 1: 2}
      - {0: 1, 1: 5}

  svc:                                # Support Vector Classifier parameters
    c_list:                           # Controls trade-off between bias and variance - Higher C = more flexibility, but higher risk of overfitting
      - 0.1
      - 1
      - 5
      - 10
      - 15
      - 20
    kernel_list:                      # Type of transformation applied to input features
      - "linear"                      # Similar to linear regression
      - "rbf"                         # Radial basis function - Effective for non-linear
      - "sigmoid"                     # Similar to neural network
    class_weight_list:                # Helps handle imbalanced class distributions by adjusting class weight
      - "balanced"
      - {0: 1, 1: 1.7}
      - {0: 1, 1: 2}
      - {0: 1, 1: 5}

  mlp:                                # Multi-layer Perceptron Classifier parameters
    hidden_layer_sizes_list:          # Number and size of hidden layers (Affects model complexity)
      - [50, 50]
      - [100, 50]
      - [100, 100]
    activation_list:                  # Determines non-linearity introduced by activation function (Affects learning capability)
      - "relu"                        # Works best for most hidden layers due to faster convergence 
      - "tanh"                        # Used in networks where centered activations are important
    solver_list:                      # Optimization algorithm
      - "adam"                        # Works better with default settings for more complex models, with faster convergence and better noisy gradient handling
      - "sgd"                         # Simple and can be effective when tuned correctly, especially for smaller or simpler models
    learning_rate_list:               # Controls how learning rate changes during training (Only for 'sgd' solver)
      - "constant"
      - "adaptive"
    max_iter_list:                    # Specifies maximum number of iterations for training
      - 500
      - 1000
      - 1500
      - 2000

  bayes:                              # BernoulliNB Classifier parameters
    alpha_list:                       # Smooth probabilities, preventing 0 probabilities for features that may no appear in training class - Higher value = more smoothing
      - 0.01
      - 0.1
      - 1.0
      - 1.5
      - 2.0
      - 3.0

  xgb:                                # XGBoost Classifier parameters
    learning_rate_list:               # Control step size at each boosting round/tree (Smaller learning rate require more trees but helps prevent overfitting)
      - 0.01
      - 0.05
      - 0.1
    max_depth_list:                   # Controls maximum depth of individual trees (Deeper trees can capture more complexity, but may overfit data)
      - 3
      - 5
      - 7
      - 10
    subsample_list:                   # Controls fraction of training data used to build each tree (Introduces randomness)
      - 0.6
      - 0.8
      - 1.0
